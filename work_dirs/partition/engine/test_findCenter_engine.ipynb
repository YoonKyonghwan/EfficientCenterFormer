{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/centerformer/det3d/core/bbox/geometry.py:149: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n",
      "/data/centerformer/det3d/core/bbox/geometry.py:162: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n",
      "/data/centerformer/det3d/core/bbox/geometry.py:280: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Convolution not built!\n",
      "Deformable Convolution not built!\n",
      "Use HM Bias:  -2.19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from det3d.torchie import Config\n",
    "from det3d.models import build_detector\n",
    "from det3d.torchie.trainer import load_checkpoint\n",
    "\n",
    "config = \"/workspace/centerformer/configs/nusc/nuscenes_centerformer_poolformer.py\"\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "checkpoint_path = \"/workspace/centerformer/work_dirs/nuscenes_poolformer/poolformer.pth\"\n",
    "\n",
    "model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_dir = \"/workspace/centerformer/work_dirs/partition/sample_data/\"\n",
    "\n",
    "with open(pickle_dir + \"findcenter_input.pkl\", 'rb') as handle:\n",
    "    x = pickle.load(handle)\n",
    "        \n",
    "with open(pickle_dir + \"findcenter_output1.pkl\", 'rb') as handle:\n",
    "    findcenter_output1 = pickle.load(handle)\n",
    "with open(pickle_dir + \"findcenter_output2.pkl\", 'rb') as handle:\n",
    "    findcenter_output2 = pickle.load(handle)\n",
    "with open(pickle_dir + \"findcenter_output3.pkl\", 'rb') as handle:\n",
    "    findcenter_output3 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/centerformer/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ct_feat, center_pos_embedding, out_scores, out_labels, out_orders, out_masks = model.neck.find_centers(x)\n",
    "    \n",
    "print((ct_feat==findcenter_output1).all())\n",
    "print((center_pos_embedding==findcenter_output2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(out_scores.shape[0]):\n",
    "    result = (out_scores[i]==findcenter_output3[i]['scores']).all()\n",
    "    result = result and (out_labels[i]==findcenter_output3[i]['labels']).all()\n",
    "    result = result and (out_orders[i]==findcenter_output3[i]['order']).all()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.5.1.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "trt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2023-14:56:13] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "def load_engine(engine_filepath):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    with open(engine_filepath, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    return engine\n",
    "\n",
    "centerFinder_engine_path = 'findCenter_folded.trt'\n",
    "cf_engine = load_engine(centerFinder_engine_path)\n",
    "cf_context = cf_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_dir + \"findcenter_input.pkl\", 'rb') as handle:\n",
    "    input_tensor = pickle.load(handle)\n",
    "print((x == input_tensor).all(), x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_feat_tensor = torch.zeros((1, 3000, 256), dtype=torch.float32, device=x.device)\n",
    "center_pos_embedding_tensor = torch.zeros((1, 3000, 256), dtype=torch.float32, device=x.device)\n",
    "out_scores_tensor = torch.zeros((6, 1, 500), dtype=torch.float32, device=x.device)\n",
    "out_labels_tensor = torch.zeros((6, 1, 500), dtype=torch.int32, device=x.device)\n",
    "out_orders_tensor = torch.zeros((6, 1, 500), dtype=torch.int32, device=x.device)\n",
    "out_masks_tensor = torch.zeros((6, 1, 500), dtype=torch.bool, device=x.device)\n",
    "IO_tensors = {\n",
    "    \"inputs\" :\n",
    "    {'input_tensor': x},\n",
    "    \"outputs\" :\n",
    "    {'ct_feat': ct_feat_tensor, 'center_pos_embedding': center_pos_embedding_tensor,\n",
    "    'out_scores': out_scores_tensor, 'out_labels': out_labels_tensor,\n",
    "    'out_orders': out_orders_tensor, 'out_masks': out_masks_tensor}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trt_engine(context, engine, tensors):\n",
    "    bindings = [None]*engine.num_bindings\n",
    "    for idx, binding in enumerate(engine):\n",
    "        tensor_name = engine.get_tensor_name(idx)\n",
    "        if engine.get_tensor_mode(binding)==trt.TensorIOMode.INPUT:\n",
    "            bindings[idx] = tensors['inputs'][tensor_name].data_ptr()\n",
    "            if context.get_tensor_shape(tensor_name):\n",
    "                context.set_input_shape(tensor_name, tensors['inputs'][tensor_name].shape)\n",
    "        else:\n",
    "            bindings[idx] = tensors['outputs'][tensor_name].data_ptr()\n",
    "    context.execute_v2(bindings=bindings)\n",
    "\n",
    "run_trt_engine(cf_context, cf_engine, IO_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((out_scores == out_scores_tensor).all())\n",
    "print((out_labels == out_labels_tensor).all())\n",
    "print((out_orders == out_orders_tensor).all())\n",
    "print((out_masks == out_masks_tensor).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.4185, 0.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.9349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7531, 0.0000, 0.0000,\n",
      "        0.0000, 0.8260], device='cuda:0')\n",
      "tensor([0.0000, 0.0000, 0.4185, 0.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.9349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7531, 0.0000, 0.0000,\n",
      "        0.0000, 0.8260], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ct_feat[0][0][:20])\n",
    "print(ct_feat_tensor[0][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/13/2023-14:56:13] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "ct_feat2\n",
      "tensor([0.0000, 0.0000, 0.4185, 0.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.9349], device='cuda:0')\n",
      "tensor([0.0000, 0.0000, 0.4185, 0.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.9349], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/centerformer/work_dirs/partition/engine/test_findCenter_engine.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63665f747274227d@ssh-remote%2Bsemo/workspace/centerformer/work_dirs/partition/engine/test_findCenter_engine.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63665f747274227d@ssh-remote%2Bsemo/workspace/centerformer/work_dirs/partition/engine/test_findCenter_engine.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     out_dict_lists \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mneck\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63665f747274227d@ssh-remote%2Bsemo/workspace/centerformer/work_dirs/partition/engine/test_findCenter_engine.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     out_dict_lists_trt \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mneck\u001b[39m.\u001b[39mforward_trt(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/centerformer/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/centerformer/det3d/models/necks/rpn_transformer_multitask.py:1057\u001b[0m, in \u001b[0;36mRPN_poolformer_multitask.forward\u001b[0;34m(self, x, example)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[39mprint\u001b[39m(out_scores[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][:\u001b[39m10\u001b[39m])\n\u001b[1;32m   1056\u001b[0m         \u001b[39mprint\u001b[39m(out_scores2[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][:\u001b[39m10\u001b[39m])\n\u001b[0;32m-> 1057\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[39mwith\u001b[39;00m nvtx\u001b[39m.\u001b[39mannotate(\u001b[39m\"\u001b[39m\u001b[39mpoolformer_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1060\u001b[0m     ct_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoolformer_forward(ct_feat, center_pos_embedding)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_dict_lists = model.neck.forward(x)\n",
    "    out_dict_lists_trt = model.neck.forward_trt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict_lists_trt[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_dict_lists[0]['ct_feat'].dtype, out_dict_lists_trt[0]['ct_feat'].dtype)\n",
    "print(out_dict_lists[0]['scores'].dtype, out_dict_lists_trt[0]['scores'].dtype)\n",
    "print(out_dict_lists[0]['labels'].dtype, out_dict_lists_trt[0]['labels'].dtype)\n",
    "print(out_dict_lists[0]['order'].dtype, out_dict_lists_trt[0]['order'].dtype)\n",
    "print(out_dict_lists[0]['mask'].dtype, out_dict_lists_trt[0]['mask'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out_dict_lists_trt)):\n",
    "    result = (out_dict_lists[i]['scores'] == out_dict_lists_trt[i]['scores']).all()\n",
    "    result = result and (out_dict_lists[i]['labels'] == out_dict_lists_trt[i]['labels']).all()\n",
    "    result = result and (out_dict_lists[i]['order'] == out_dict_lists_trt[i]['order']).all()\n",
    "    result = result and (out_dict_lists[i]['mask'] == out_dict_lists_trt[i]['mask']).all()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_dict_lists[0]['ct_feat'][0][0][:20])\n",
    "print(out_dict_lists_trt[0]['ct_feat'][0][0][:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centerformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
