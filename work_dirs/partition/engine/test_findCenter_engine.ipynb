{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/centerformer/det3d/core/bbox/geometry.py:149: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n",
      "/data/centerformer/det3d/core/bbox/geometry.py:162: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit(nopython=False)\n",
      "/data/centerformer/det3d/core/bbox/geometry.py:280: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Convolution not built!\n",
      "Deformable Convolution not built!\n",
      "[12/14/2023-06:42:46] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "Use HM Bias:  -2.19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from det3d.torchie import Config\n",
    "from det3d.models import build_detector\n",
    "from det3d.torchie.trainer import load_checkpoint\n",
    "\n",
    "config = \"/workspace/centerformer/configs/nusc/nuscenes_centerformer_poolformer.py\"\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "checkpoint_path = \"/workspace/centerformer/work_dirs/nuscenes_poolformer/poolformer.pth\"\n",
    "\n",
    "model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_dir = \"/workspace/centerformer/work_dirs/partition/sample_data/\"\n",
    "\n",
    "with open(pickle_dir + \"findcenter_input.pkl\", 'rb') as handle:\n",
    "    x = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/centerformer/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ct_feat, center_pos_embedding, out_scores, out_labels, out_orders, out_masks = model.neck.find_centers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.5.1.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "trt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2023-06:42:48] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "def load_engine(engine_filepath):\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    with open(engine_filepath, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    return engine\n",
    "\n",
    "centerFinder_engine_path = '/workspace/centerformer/work_dirs/partition/engine/findCenter_folded_op17_v1.trt'\n",
    "cf_engine = load_engine(centerFinder_engine_path)\n",
    "cf_context = cf_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True, device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_dir + \"findcenter_input.pkl\", 'rb') as handle:\n",
    "    input_tensor = pickle.load(handle)\n",
    "print((x == input_tensor).all(), x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 180, 180])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_device = torch.device(\"cuda\")\n",
    "ct_feat_tensor = torch.zeros((1, 3000, 256), dtype=torch.float32, device=to_device)\n",
    "center_pos_embedding_tensor = torch.zeros((1, 3000, 256), dtype=torch.float32, device=to_device)\n",
    "out_scores_tensor = torch.zeros((6, 1, 500), dtype=torch.float32, device=to_device)\n",
    "out_labels_tensor = torch.zeros((6, 1, 500), dtype=torch.int32, device=to_device)\n",
    "out_orders_tensor = torch.zeros((6, 1, 500), dtype=torch.int32, device=to_device)\n",
    "out_masks_tensor = torch.zeros((6, 1, 500), dtype=torch.bool, device=to_device)\n",
    "IO_tensors = {\n",
    "    \"inputs\" :\n",
    "    {'input_tensor': input_tensor},\n",
    "    \"outputs\" :{\n",
    "        'out_labels': out_labels_tensor,\n",
    "        'ct_feat': ct_feat_tensor, \n",
    "        'center_pos_embedding': center_pos_embedding_tensor,\n",
    "        'out_scores': out_scores_tensor, \n",
    "        'out_orders': out_orders_tensor, \n",
    "        'out_masks': out_masks_tensor}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2023-06:43:18] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    }
   ],
   "source": [
    "def run_trt_engine(context, engine, tensors):\n",
    "    bindings = [None]*engine.num_bindings\n",
    "    for idx, binding in enumerate(engine):\n",
    "        tensor_name = engine.get_tensor_name(idx)\n",
    "        if engine.get_tensor_mode(binding)==trt.TensorIOMode.INPUT:\n",
    "            bindings[idx] = tensors['inputs'][tensor_name].data_ptr()\n",
    "            if context.get_tensor_shape(tensor_name):\n",
    "                context.set_input_shape(tensor_name, tensors['inputs'][tensor_name].shape)\n",
    "        else:\n",
    "            bindings[idx] = tensors['outputs'][tensor_name].data_ptr()\n",
    "    context.execute_v2(bindings=bindings)\n",
    "\n",
    "run_trt_engine(cf_context, cf_engine, IO_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8610, 0.8600, 0.8393, 0.8083, 0.8044, 0.7641, 0.7413, 0.7288, 0.7048,\n",
      "        0.6847], device='cuda:0')\n",
      "tensor([0.0350, 0.0307, 0.0302, 0.0298, 0.0294, 0.0291, 0.0288, 0.0285, 0.0283,\n",
      "        0.0280], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(out_scores[0][0][:10])\n",
    "print(out_scores_tensor[0][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.4185, 0.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.9349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7531, 0.0000, 0.0000,\n",
      "        0.0000, 0.8260], device='cuda:0')\n",
      "tensor([0.3978, 0.5673, 0.1729, 0.2748, 0.4839, 0.0000, 0.4179, 0.0000, 0.1268,\n",
      "        0.1202, 0.0856, 0.4713, 0.3143, 0.3502, 0.1196, 0.1893, 0.6793, 0.8308,\n",
      "        0.0000, 0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ct_feat[0][0][:20])\n",
    "print(ct_feat_tensor[0][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49866, 62040, 59169, 57018, 61680, 98445, 59170, 50226, 30777, 57017,\n",
      "        59529, 98446, 59530, 27541, 62041, 49867, 61679, 30776, 31137, 49506],\n",
      "       device='cuda:0')\n",
      "tensor([   359,   7200,      0,   8640,   5760,   7560,  10080,   7920,   8280,\n",
      "         11520,  10800,   9360,   9000, 118080,  12960, 103680,  44640,  84960,\n",
      "         86400,  43200], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(out_orders[0][0][:20])\n",
    "print(out_orders_tensor[0][0][:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centerformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
