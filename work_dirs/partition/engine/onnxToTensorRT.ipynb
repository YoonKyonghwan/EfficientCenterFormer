{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import os, sys\n",
    "\n",
    "def build_engine(model_file, shapes, max_ws=512*1024*1024, fp16=False, timing_cache=None, faster_dynamic_shapes=False):\n",
    "    if faster_dynamic_shapes and float(trt.__version__[:3]) < 8.5:\n",
    "        print(\"Faster dynamic shapes preview feature is only supported on TRT 8.5+\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "    # config.max_workspace_size = max_ws\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_ws)\n",
    "    if fp16:\n",
    "        config.flags |= 1 << int(trt.BuilderFlag.FP16)\n",
    "    config.set_preview_feature(trt.PreviewFeature.FASTER_DYNAMIC_SHAPES_0805, faster_dynamic_shapes)\n",
    "    profile = builder.create_optimization_profile()\n",
    "    for s in shapes:\n",
    "        profile.set_shape(s['name'], min=s['min'], opt=s['opt'], max=s['max'])\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    timing_cache_available = int(trt.__version__[0]) >= 8 and timing_cache != None\n",
    "    # load global timing cache\n",
    "    if timing_cache_available:\n",
    "        if os.path.exists(timing_cache):\n",
    "            with open(timing_cache, \"rb\") as f:\n",
    "                cache = config.create_timing_cache(f.read())\n",
    "                config.set_timing_cache(cache, ignore_mismatch = False)\n",
    "        else:\n",
    "            cache = config.create_timing_cache(b\"\")\n",
    "            config.set_timing_cache(cache, ignore_mismatch = False)\n",
    "\n",
    "    explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(explicit_batch)\n",
    "\n",
    "    with trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        with open(model_file, 'rb') as model:\n",
    "            parsed = parser.parse(model.read())\n",
    "            for i in range(parser.num_errors):\n",
    "                print(\"TensorRT ONNX parser error:\", parser.get_error(i))\n",
    "            # engine = builder.build_engine(network, config=config)\n",
    "            engine = builder.build_serialized_network(network, config=config)\n",
    "\n",
    "            # save global timing cache\n",
    "            if timing_cache_available:\n",
    "                cache = config.get_timing_cache()\n",
    "                with cache.serialize() as buffer:\n",
    "                    with open(timing_cache, \"wb\") as f:\n",
    "                        f.write(buffer)\n",
    "                        f.flush()\n",
    "                        os.fsync(f)\n",
    "\n",
    "            return engine\n",
    "        \n",
    "def load_engine(engine_filepath, trt_logger):\n",
    "    with open(engine_filepath, \"rb\") as f, trt.Runtime(trt_logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2023-01:58:18] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[12/14/2023-01:58:18] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-01:58:18] [TRT] [E] Output tensor out_masks of type Float produced from output of incompatible type Bool\n",
      "[12/14/2023-01:58:19] [TRT] [W] Using kFASTER_DYNAMIC_SHAPES_0805 preview feature.\n",
      "[12/14/2023-02:00:03] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[12/14/2023-02:00:03] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[12/14/2023-02:00:03] [TRT] [W] - 40 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[12/14/2023-02:00:03] [TRT] [W] - 15 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'findCenter_folded_op17_v1'\n",
    "onnx_path = '../onnx/'+model_name+'.onnx'\n",
    "static_shapes=[{\"name\": \"input_tensor\", \"min\": (1, 256, 180, 180), \"opt\": (1, 256, 180, 180), \"max\": (1, 256, 180, 180)}]\n",
    "static_engine = build_engine(onnx_path, shapes=static_shapes, faster_dynamic_shapes=True, fp16=True)\n",
    "engine_path = model_name+'.trt'\n",
    "with open(engine_path, 'wb') as f:\n",
    "    f.write(static_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/14/2023-02:00:03] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[12/14/2023-02:00:03] [TRT] [E] Output tensor out_masks of type Float produced from output of incompatible type Bool\n",
      "[12/14/2023-02:00:03] [TRT] [W] Using kFASTER_DYNAMIC_SHAPES_0805 preview feature.\n",
      "[12/14/2023-02:01:09] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[12/14/2023-02:01:09] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[12/14/2023-02:01:09] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[12/14/2023-02:01:09] [TRT] [W] - 40 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[12/14/2023-02:01:09] [TRT] [W] - 15 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'findCenter_folded_op11_v1'\n",
    "onnx_path = '../onnx/'+model_name+'.onnx'\n",
    "static_shapes=[{\"name\": \"input_tensor\", \"min\": (1, 256, 180, 180), \"opt\": (1, 256, 180, 180), \"max\": (1, 256, 180, 180)}]\n",
    "static_engine = build_engine(onnx_path, shapes=static_shapes, faster_dynamic_shapes=True, fp16=True)\n",
    "engine_path = model_name+'.trt'\n",
    "with open(engine_path, 'wb') as f:\n",
    "    f.write(static_engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
